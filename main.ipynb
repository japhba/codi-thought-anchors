{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODI Model vs NLP Chain-of-Thought Comparison\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **CODI Model**: Compresses chain-of-thought into latent continuous space\n",
    "2. **NLP-based Model**: Standard model with explicit natural language chain-of-thought\n",
    "\n",
    "We'll extract hidden activations and latent/NLP traces from both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'codi')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "from src.model import CODI, ModelArguments, TrainingArguments\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Download CODI checkpoint and configure models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CODI-gpt2 checkpoint from HuggingFace\n",
    "ckpt_dir = \"codi_gpt2_ckpt\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    ckpt_path = hf_hub_download(\n",
    "        repo_id=\"zen-E/CODI-gpt2\",\n",
    "        filename=\"model.safetensors\",\n",
    "        local_dir=ckpt_dir\n",
    "    )\n",
    "    print(f\"Downloaded checkpoint to: {ckpt_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not download checkpoint: {e}\")\n",
    "    print(\"Will use randomly initialized model for demonstration\")\n",
    "    ckpt_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model arguments\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"gpt2\",\n",
    "    lora_r=128,\n",
    "    lora_alpha=32,\n",
    "    lora_init=True,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "# Create a minimal training args for model initialization\n",
    "class MinimalTrainingArgs:\n",
    "    bf16 = True\n",
    "    num_latent = 6\n",
    "    use_lora = True\n",
    "    use_prj = True\n",
    "    prj_dim = 768\n",
    "    prj_dropout = 0.0\n",
    "    prj_no_ln = False\n",
    "    restore_from = \"\"\n",
    "    inf_latent_iterations = 6\n",
    "    remove_eos = True\n",
    "    fix_attn_mask = False\n",
    "    print_loss = False\n",
    "    distill_loss_div_std = False\n",
    "    distill_loss_type = \"smooth_l1\"\n",
    "    distill_loss_factor = 1.0\n",
    "    ref_loss_factor = 1.0\n",
    "\n",
    "training_args = MinimalTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI Model (Latent Thought Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=True,\n",
    "    r=model_args.lora_r,\n",
    "    lora_alpha=model_args.lora_alpha,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "    init_lora_weights=True,\n",
    ")\n",
    "\n",
    "# Initialize CODI model\n",
    "codi_model = CODI(model_args, training_args, lora_config)\n",
    "\n",
    "# Load checkpoint if available\n",
    "if ckpt_path and os.path.exists(ckpt_path):\n",
    "    from safetensors.torch import load_file\n",
    "    state_dict = load_file(ckpt_path)\n",
    "    codi_model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"Loaded CODI checkpoint\")\n",
    "else:\n",
    "    print(\"Using randomly initialized CODI model\")\n",
    "\n",
    "codi_model = codi_model.to(device)\n",
    "codi_model.eval()\n",
    "print(f\"CODI model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NLP-based Model (Explicit Chain-of-Thought)\n",
    "\n",
    "The NLP cousin uses the same base GPT-2 but processes explicit natural language chain-of-thought instead of latent thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base GPT-2 for NLP chain-of-thought (no latent compression)\n",
    "nlp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    torch_dtype=torch.bfloat16 if training_args.bf16 else torch.float32,\n",
    ")\n",
    "nlp_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "nlp_tokenizer.pad_token = nlp_tokenizer.eos_token\n",
    "\n",
    "nlp_model = nlp_model.to(device)\n",
    "nlp_model.eval()\n",
    "print(f\"NLP model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Query and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample math query (GSM8K style)\n",
    "query = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning. How many eggs does she have left each day?\"\n",
    "\n",
    "# Chain-of-thought prompt for NLP model\n",
    "cot_prompt = f\"\"\"Question: {query}\n",
    "\n",
    "Let me solve this step by step:\n",
    "Step 1: Janet's ducks lay 16 eggs per day.\n",
    "Step 2: She eats 3 eggs for breakfast.\n",
    "Step 3: Eggs left = 16 - 3 = 13\n",
    "\n",
    "The answer is 13.\"\"\"\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nCoT Prompt for NLP model:\")\n",
    "print(cot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states_summary(hidden_states):\n",
    "    \"\"\"Extract summary statistics from hidden states.\"\"\"\n",
    "    summary = {\n",
    "        \"num_layers\": len(hidden_states),\n",
    "        \"shape_per_layer\": hidden_states[0].shape,\n",
    "        \"last_layer_mean\": hidden_states[-1].mean().item(),\n",
    "        \"last_layer_std\": hidden_states[-1].std().item(),\n",
    "        \"last_token_embedding\": hidden_states[-1][:, -1, :].detach().cpu(),\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "def decode_latent_to_tokens(model, latent_embd, tokenizer, top_k=5):\n",
    "    \"\"\"Probe what tokens the latent embedding represents.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Get logits by passing through lm_head\n",
    "        if hasattr(model, 'codi'):\n",
    "            logits = model.codi.lm_head(latent_embd)\n",
    "        else:\n",
    "            logits = model.lm_head(latent_embd)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs, k=top_k, dim=-1)\n",
    "        \n",
    "        decoded = []\n",
    "        for i in range(top_indices.shape[1]):\n",
    "            tokens = [tokenizer.decode([idx.item()]) for idx in top_indices[0, i]]\n",
    "            probs_list = top_probs[0, i].tolist()\n",
    "            decoded.append(list(zip(tokens, probs_list)))\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CODI Model: Extract Latent Traces and Hidden Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input for CODI\n",
    "codi_tokenizer = codi_model.tokenizer\n",
    "inputs = codi_tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Add BOT (beginning of thought) token\n",
    "if training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([[codi_model.bot_id]], dtype=torch.long, device=device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([[codi_tokenizer.eos_token_id, codi_model.bot_id]], dtype=torch.long, device=device)\n",
    "\n",
    "input_ids = torch.cat([inputs[\"input_ids\"], bot_tensor], dim=1)\n",
    "attention_mask = torch.cat([inputs[\"attention_mask\"], torch.ones_like(bot_tensor)], dim=1)\n",
    "\n",
    "print(f\"Input tokens: {codi_tokenizer.decode(input_ids[0])}\")\n",
    "print(f\"Input shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CODI model and collect latent traces\n",
    "codi_latent_traces = []\n",
    "codi_hidden_activations = []\n",
    "codi_decoded_latents = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding\n",
    "    outputs = codi_model.codi(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # Extract initial hidden state (last token = BOT position)\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "    codi_latent_traces.append(latent_embd.clone())\n",
    "    codi_hidden_activations.append(extract_hidden_states_summary(outputs.hidden_states))\n",
    "    codi_decoded_latents.append(decode_latent_to_tokens(codi_model, latent_embd, codi_tokenizer))\n",
    "    \n",
    "    # Apply projection if used\n",
    "    if training_args.use_prj:\n",
    "        latent_embd = codi_model.prj(latent_embd)\n",
    "    \n",
    "    # Iterate through latent thought steps\n",
    "    for i in range(training_args.inf_latent_iterations):\n",
    "        outputs = codi_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1, :].unsqueeze(1)\n",
    "        \n",
    "        # Store traces before projection\n",
    "        codi_latent_traces.append(latent_embd.clone())\n",
    "        codi_hidden_activations.append(extract_hidden_states_summary(outputs.hidden_states))\n",
    "        codi_decoded_latents.append(decode_latent_to_tokens(codi_model, latent_embd, codi_tokenizer))\n",
    "        \n",
    "        if training_args.use_prj:\n",
    "            latent_embd = codi_model.prj(latent_embd)\n",
    "\n",
    "print(f\"Collected {len(codi_latent_traces)} latent traces\")\n",
    "print(f\"Latent embedding shape: {codi_latent_traces[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CODI latent traces (decoded to most likely tokens)\n",
    "print(\"=\" * 60)\n",
    "print(\"CODI LATENT TRACES (Decoded to top-5 tokens)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, decoded in enumerate(codi_decoded_latents):\n",
    "    step_name = \"Initial (BOT)\" if i == 0 else f\"Latent Step {i}\"\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"  Top tokens: {decoded[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display CODI hidden activation statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"CODI HIDDEN ACTIVATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, activation in enumerate(codi_hidden_activations):\n",
    "    step_name = \"Initial\" if i == 0 else f\"Latent Step {i}\"\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"  Num layers: {activation['num_layers']}\")\n",
    "    print(f\"  Last layer mean: {activation['last_layer_mean']:.4f}\")\n",
    "    print(f\"  Last layer std: {activation['last_layer_std']:.4f}\")\n",
    "    print(f\"  Last token embedding norm: {activation['last_token_embedding'].norm().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NLP Model: Extract Chain-of-Thought Traces and Hidden Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize CoT prompt for NLP model\n",
    "nlp_inputs = nlp_tokenizer(cot_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "print(f\"NLP input length: {nlp_inputs['input_ids'].shape[1]} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NLP model with explicit CoT\n",
    "with torch.no_grad():\n",
    "    nlp_outputs = nlp_model(\n",
    "        input_ids=nlp_inputs[\"input_ids\"],\n",
    "        attention_mask=nlp_inputs[\"attention_mask\"],\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "nlp_hidden_summary = extract_hidden_states_summary(nlp_outputs.hidden_states)\n",
    "print(f\"NLP model hidden states collected\")\n",
    "print(f\"  Num layers: {nlp_hidden_summary['num_layers']}\")\n",
    "print(f\"  Shape per layer: {nlp_hidden_summary['shape_per_layer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract NLP traces at key positions (each CoT step)\n",
    "cot_steps = [\n",
    "    \"Question:\",\n",
    "    \"Step 1:\",\n",
    "    \"Step 2:\",\n",
    "    \"Step 3:\",\n",
    "    \"The answer is\"\n",
    "]\n",
    "\n",
    "nlp_traces = []\n",
    "tokens = nlp_tokenizer.encode(cot_prompt)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NLP CHAIN-OF-THOUGHT TRACES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for step in cot_steps:\n",
    "    # Find position of this step in the tokenized input\n",
    "    step_tokens = nlp_tokenizer.encode(step, add_special_tokens=False)\n",
    "    \n",
    "    # Search for the step in the token sequence\n",
    "    for pos in range(len(tokens) - len(step_tokens) + 1):\n",
    "        if tokens[pos:pos+len(step_tokens)] == step_tokens:\n",
    "            # Get hidden state at this position\n",
    "            hidden = nlp_outputs.hidden_states[-1][0, pos + len(step_tokens) - 1, :]\n",
    "            nlp_traces.append({\n",
    "                \"step\": step,\n",
    "                \"position\": pos,\n",
    "                \"hidden_norm\": hidden.norm().item(),\n",
    "                \"hidden_mean\": hidden.mean().item(),\n",
    "                \"hidden_std\": hidden.std().item(),\n",
    "                \"embedding\": hidden.cpu()\n",
    "            })\n",
    "            print(f\"\\n{step} (position {pos}):\")\n",
    "            print(f\"  Hidden norm: {hidden.norm().item():.4f}\")\n",
    "            print(f\"  Hidden mean: {hidden.mean().item():.4f}\")\n",
    "            print(f\"  Hidden std: {hidden.std().item():.4f}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare CODI vs NLP Hidden Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compare latent norms across steps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CODI latent norms\n",
    "codi_norms = [trace.norm().item() for trace in codi_latent_traces]\n",
    "axes[0].bar(range(len(codi_norms)), codi_norms, color='blue', alpha=0.7)\n",
    "axes[0].set_xlabel('Latent Step')\n",
    "axes[0].set_ylabel('Embedding Norm')\n",
    "axes[0].set_title('CODI: Latent Embedding Norms')\n",
    "axes[0].set_xticks(range(len(codi_norms)))\n",
    "axes[0].set_xticklabels(['Init'] + [f'L{i+1}' for i in range(len(codi_norms)-1)])\n",
    "\n",
    "# NLP CoT norms\n",
    "if nlp_traces:\n",
    "    nlp_norms = [t['hidden_norm'] for t in nlp_traces]\n",
    "    nlp_labels = [t['step'][:10] for t in nlp_traces]\n",
    "    axes[1].bar(range(len(nlp_norms)), nlp_norms, color='green', alpha=0.7)\n",
    "    axes[1].set_xlabel('CoT Step')\n",
    "    axes[1].set_ylabel('Embedding Norm')\n",
    "    axes[1].set_title('NLP: Chain-of-Thought Embedding Norms')\n",
    "    axes[1].set_xticks(range(len(nlp_norms)))\n",
    "    axes[1].set_xticklabels(nlp_labels, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hidden state evolution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CODI hidden states (first 50 dimensions)\n",
    "codi_hidden_matrix = torch.stack([t.squeeze() for t in codi_latent_traces]).cpu().numpy()\n",
    "im1 = axes[0].imshow(codi_hidden_matrix[:, :50], aspect='auto', cmap='viridis')\n",
    "axes[0].set_xlabel('Hidden Dimension (first 50)')\n",
    "axes[0].set_ylabel('Latent Step')\n",
    "axes[0].set_title('CODI: Latent Activations Evolution')\n",
    "axes[0].set_yticks(range(len(codi_latent_traces)))\n",
    "axes[0].set_yticklabels(['Init'] + [f'L{i+1}' for i in range(len(codi_latent_traces)-1)])\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# NLP hidden states at CoT positions\n",
    "if nlp_traces:\n",
    "    nlp_hidden_matrix = torch.stack([t['embedding'] for t in nlp_traces]).numpy()\n",
    "    im2 = axes[1].imshow(nlp_hidden_matrix[:, :50], aspect='auto', cmap='viridis')\n",
    "    axes[1].set_xlabel('Hidden Dimension (first 50)')\n",
    "    axes[1].set_ylabel('CoT Step')\n",
    "    axes[1].set_title('NLP: Chain-of-Thought Activations')\n",
    "    axes[1].set_yticks(range(len(nlp_traces)))\n",
    "    axes[1].set_yticklabels([t['step'][:10] for t in nlp_traces])\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Raw Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all collected data\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY: Available Data\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. CODI Latent Traces:\")\n",
    "print(f\"   - Number of traces: {len(codi_latent_traces)}\")\n",
    "print(f\"   - Shape per trace: {codi_latent_traces[0].shape}\")\n",
    "print(f\"   - Access: codi_latent_traces[i] for raw tensor\")\n",
    "\n",
    "print(\"\\n2. CODI Hidden Activations:\")\n",
    "print(f\"   - Number of snapshots: {len(codi_hidden_activations)}\")\n",
    "print(f\"   - Layers per snapshot: {codi_hidden_activations[0]['num_layers']}\")\n",
    "print(f\"   - Access: codi_hidden_activations[i]['last_token_embedding']\")\n",
    "\n",
    "print(\"\\n3. CODI Decoded Latents (token interpretations):\")\n",
    "print(f\"   - Access: codi_decoded_latents[i] for top-k tokens\")\n",
    "\n",
    "print(\"\\n4. NLP Chain-of-Thought Traces:\")\n",
    "print(f\"   - Number of CoT steps captured: {len(nlp_traces)}\")\n",
    "print(f\"   - Access: nlp_traces[i]['embedding'] for hidden state\")\n",
    "print(f\"   - Access: nlp_traces[i]['step'] for step name\")\n",
    "\n",
    "print(\"\\n5. Full NLP Hidden States:\")\n",
    "print(f\"   - Access: nlp_outputs.hidden_states for all layers\")\n",
    "print(f\"   - Shape: {nlp_outputs.hidden_states[-1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access raw latent embeddings\n",
    "print(\"Example CODI latent (step 3):\")\n",
    "print(codi_latent_traces[3].squeeze()[:10])  # First 10 dimensions\n",
    "\n",
    "print(\"\\nExample NLP hidden state (Step 2):\")\n",
    "if len(nlp_traces) > 2:\n",
    "    print(nlp_traces[2]['embedding'][:10])  # First 10 dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
